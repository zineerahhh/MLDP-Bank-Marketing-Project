{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Term Deposit Prediction\n",
    "\n",
    "## Project Overview\n",
    "**Business Problem:** Banks spend significant resources on marketing campaigns to convince customers to subscribe to term deposits. Predicting which customers are likely to subscribe allows banks to target their marketing efforts more effectively, reducing costs and improving conversion rates.\n",
    "\n",
    "**ML Task:** Binary Classification (Predict if a customer will subscribe to a term deposit: Yes/No)\n",
    "\n",
    "**Dataset:** UCI Bank Marketing Dataset (bank-additional-full.csv)\n",
    "- Source: https://archive.ics.uci.edu/dataset/222/bank+marketing\n",
    "- Samples: 41,188 rows\n",
    "- Features: 20 input features + 1 target variable\n",
    "\n",
    "**Key Challenge:** The dataset is highly imbalanced (~88% No, ~12% Yes), requiring techniques like SMOTE to handle class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn for preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "\n",
    "# Imbalanced-learn for SMOTE\n",
    "from imblearn.pipeline import Pipeline  # IMPORTANT: use imblearn's Pipeline (not sklearn's)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    roc_auc_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (41188, 21)\n",
      "Total Samples: 41,188\n",
      "Total Features: 21\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (semicolon-separated)\n",
    "df = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Samples: {df.shape[0]:,}\")\n",
    "print(f\"Total Features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data types and info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41188.00000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.02406</td>\n",
       "      <td>258.285010</td>\n",
       "      <td>2.567593</td>\n",
       "      <td>962.475454</td>\n",
       "      <td>0.172963</td>\n",
       "      <td>0.081886</td>\n",
       "      <td>93.575664</td>\n",
       "      <td>-40.502600</td>\n",
       "      <td>3.621291</td>\n",
       "      <td>5167.035911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.42125</td>\n",
       "      <td>259.279249</td>\n",
       "      <td>2.770014</td>\n",
       "      <td>186.910907</td>\n",
       "      <td>0.494901</td>\n",
       "      <td>1.570960</td>\n",
       "      <td>0.578840</td>\n",
       "      <td>4.628198</td>\n",
       "      <td>1.734447</td>\n",
       "      <td>72.251528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>92.201000</td>\n",
       "      <td>-50.800000</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>4963.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>93.075000</td>\n",
       "      <td>-42.700000</td>\n",
       "      <td>1.344000</td>\n",
       "      <td>5099.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>93.749000</td>\n",
       "      <td>-41.800000</td>\n",
       "      <td>4.857000</td>\n",
       "      <td>5191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.00000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>93.994000</td>\n",
       "      <td>-36.400000</td>\n",
       "      <td>4.961000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>98.00000</td>\n",
       "      <td>4918.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>94.767000</td>\n",
       "      <td>-26.900000</td>\n",
       "      <td>5.045000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age      duration      campaign         pdays      previous  \\\n",
       "count  41188.00000  41188.000000  41188.000000  41188.000000  41188.000000   \n",
       "mean      40.02406    258.285010      2.567593    962.475454      0.172963   \n",
       "std       10.42125    259.279249      2.770014    186.910907      0.494901   \n",
       "min       17.00000      0.000000      1.000000      0.000000      0.000000   \n",
       "25%       32.00000    102.000000      1.000000    999.000000      0.000000   \n",
       "50%       38.00000    180.000000      2.000000    999.000000      0.000000   \n",
       "75%       47.00000    319.000000      3.000000    999.000000      0.000000   \n",
       "max       98.00000   4918.000000     56.000000    999.000000      7.000000   \n",
       "\n",
       "       emp.var.rate  cons.price.idx  cons.conf.idx     euribor3m   nr.employed  \n",
       "count  41188.000000    41188.000000   41188.000000  41188.000000  41188.000000  \n",
       "mean       0.081886       93.575664     -40.502600      3.621291   5167.035911  \n",
       "std        1.570960        0.578840       4.628198      1.734447     72.251528  \n",
       "min       -3.400000       92.201000     -50.800000      0.634000   4963.600000  \n",
       "25%       -1.800000       93.075000     -42.700000      1.344000   5099.100000  \n",
       "50%        1.100000       93.749000     -41.800000      4.857000   5191.000000  \n",
       "75%        1.400000       93.994000     -36.400000      4.961000   5228.100000  \n",
       "max        1.400000       94.767000     -26.900000      5.045000   5228.100000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for categorical columns\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for 'unknown' values in categorical columns (these are essentially missing values)\n",
    "print(\"'Unknown' values in categorical columns:\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    unknown_count = (df[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        print(f\"  {col}: {unknown_count} ({unknown_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Target Variable Distribution (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"Target Distribution:\")\n",
    "print(df['y'].value_counts())\n",
    "print(f\"\\nPercentage:\")\n",
    "print(df['y'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "df['y'].value_counts().plot(kind='bar', color=colors, edgecolor='black', ax=ax)\n",
    "ax.set_title('Target Variable Distribution (Imbalanced Dataset)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Subscribed to Term Deposit', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (count, pct) in enumerate(zip(df['y'].value_counts(), df['y'].value_counts(normalize=True)*100)):\n",
    "    ax.text(i, count + 500, f'{count:,}\\n({pct:.1f}%)', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️ OBSERVATION: Dataset is highly IMBALANCED (~88% No vs ~12% Yes)\")\n",
    "print(\"   → Will use SMOTE to handle this imbalance during model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns (excluding target)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "key_numerical = ['age', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.conf.idx']\n",
    "\n",
    "for i, col in enumerate(key_numerical):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    df[col].hist(bins=30, ax=ax, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'Distribution of {col}', fontsize=12)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Distribution of Key Numerical Features', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscription rate by job type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Job vs Target\n",
    "job_target = df.groupby('job')['y'].apply(lambda x: (x == 'yes').mean() * 100).sort_values(ascending=False)\n",
    "job_target.plot(kind='bar', ax=axes[0], color='teal', edgecolor='black')\n",
    "axes[0].set_title('Subscription Rate by Job Type', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Job')\n",
    "axes[0].set_ylabel('Subscription Rate (%)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Education vs Target\n",
    "edu_target = df.groupby('education')['y'].apply(lambda x: (x == 'yes').mean() * 100).sort_values(ascending=False)\n",
    "edu_target.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_title('Subscription Rate by Education Level', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Education')\n",
    "axes[1].set_ylabel('Subscription Rate (%)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATION: Students and retired people have higher subscription rates.\")\n",
    "print(\"             Education level also influences subscription likelihood.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous campaign outcome (poutcome) vs current subscription\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "poutcome_target = df.groupby('poutcome')['y'].apply(lambda x: (x == 'yes').mean() * 100).sort_values(ascending=False)\n",
    "poutcome_target.plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c', '#95a5a6'], edgecolor='black')\n",
    "ax.set_title('Subscription Rate by Previous Campaign Outcome', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Previous Outcome')\n",
    "ax.set_ylabel('Subscription Rate (%)')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "for i, v in enumerate(poutcome_target):\n",
    "    ax.text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATION: Customers who previously said 'yes' (success) have a MUCH higher subscription rate (~65%)!\")\n",
    "print(\"             This is a very strong predictor - 'poutcome' is an important feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target to numeric for correlation\n",
    "df_corr = df.copy()\n",
    "df_corr['y_numeric'] = (df_corr['y'] == 'yes').astype(int)\n",
    "\n",
    "# Correlation matrix for numerical features with target\n",
    "numerical_for_corr = numerical_cols + ['y_numeric']\n",
    "corr_matrix = df_corr[numerical_for_corr].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, fmt='.2f', \n",
    "            linewidths=0.5, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix of Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target\n",
    "print(\"\\nCorrelation with Target (y):\")\n",
    "target_corr = corr_matrix['y_numeric'].drop('y_numeric').sort_values(key=abs, ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 EDA Summary & Insights\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Class Imbalance:** ~88% No vs ~12% Yes - requires SMOTE or similar technique\n",
    "2. **Important Features:** \n",
    "   - `poutcome` (previous campaign outcome) is a strong predictor\n",
    "   - Economic indicators (`emp.var.rate`, `euribor3m`, `nr.employed`) show correlation with target\n",
    "   - `duration` has high correlation but causes data leakage (will be dropped)\n",
    "3. **Data Quality:** No missing values, but 'unknown' values exist in some categorical columns\n",
    "4. **Feature Types:** Mix of numerical and categorical features requiring appropriate preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Selection & Target Preparation\n",
    "\n",
    "**Important:** We drop `duration` because it causes data leakage - this value is only known after the call ends, so it cannot be used to predict whether a customer will subscribe before calling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "data = df.copy()\n",
    "\n",
    "# Encode target variable: 'yes' -> 1, 'no' -> 0\n",
    "data['target'] = data['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Drop 'duration' (data leakage) and original 'y' column\n",
    "# Duration is known only after call ends - cannot be used for prediction\n",
    "data = data.drop(columns=['y', 'duration'])\n",
    "\n",
    "print(f\"Shape after dropping 'duration' and 'y': {data.shape}\")\n",
    "print(f\"\\nTarget distribution after encoding:\")\n",
    "print(data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train-Test Split\n",
    "\n",
    "**Important:** Split data BEFORE applying SMOTE to prevent data leakage. SMOTE should only be applied to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "# stratify=y ensures both sets have similar class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor with ColumnTransformer\n",
    "# - Numerical features: StandardScaler (normalize to mean=0, std=1)\n",
    "# - Categorical features: OneHotEncoder (convert to binary columns)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor created:\")\n",
    "print(\"  - Numerical features: StandardScaler\")\n",
    "print(\"  - Categorical features: OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Verify Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the preprocessor\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "print(f\"Shape after preprocessing: {X_train_preprocessed.shape}\")\n",
    "print(f\"Original features: {X_train.shape[1]} → Preprocessed features: {X_train_preprocessed.shape[1]}\")\n",
    "print(\"\\n(Increase is due to One-Hot Encoding of categorical variables)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Handling Class Imbalance with SMOTE\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** creates synthetic samples of the minority class by:\n",
    "1. Finding k-nearest neighbors for minority class samples\n",
    "2. Creating new samples along the line between the sample and its neighbors\n",
    "\n",
    "This helps the model learn better decision boundaries for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE\n",
    "# sampling_strategy='auto' balances classes to 1:1 ratio\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5)\n",
    "\n",
    "# Demonstrate SMOTE effect on training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "print(\"SMOTE Results:\")\n",
    "print(f\"  Before SMOTE: {X_train_preprocessed.shape[0]:,} samples\")\n",
    "print(f\"  After SMOTE:  {X_train_smote.shape[0]:,} samples\")\n",
    "print(f\"\\nClass distribution before SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SMOTE effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "y_train.value_counts().plot(kind='bar', ax=axes[0], color=['#ff6b6b', '#4ecdc4'], edgecolor='black')\n",
    "axes[0].set_title('Before SMOTE', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
    "\n",
    "# After SMOTE\n",
    "pd.Series(y_train_smote).value_counts().plot(kind='bar', ax=axes[1], color=['#ff6b6b', '#4ecdc4'], edgecolor='black')\n",
    "axes[1].set_title('After SMOTE', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
    "\n",
    "plt.suptitle('Effect of SMOTE on Class Distribution', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Development\n",
    "\n",
    "We will train and compare two classification models:\n",
    "1. **Logistic Regression** - Simple, interpretable baseline model\n",
    "2. **Random Forest** - Ensemble model that often performs well on tabular data\n",
    "\n",
    "**Evaluation Metric:** We use **ROC-AUC** as our primary metric because:\n",
    "- It's threshold-independent\n",
    "- Works well with imbalanced datasets\n",
    "- Measures the model's ability to distinguish between classes\n",
    "- For business context: Higher AUC means better identification of potential subscribers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with preprocessing, SMOTE, and Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression...\")\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "y_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOGISTIC REGRESSION - BASELINE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_proba_lr):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lr, display_labels=['No', 'Yes'], \n",
    "                                        cmap='Blues', ax=ax)\n",
    "ax.set_title('Logistic Regression - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with preprocessing, SMOTE, and Random Forest\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Random Forest...\")\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "y_proba_rf = pipeline_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST - BASELINE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Random Forest\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf, display_labels=['No', 'Yes'], \n",
    "                                        cmap='Greens', ax=ax)\n",
    "ax.set_title('Random Forest - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Comparison (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'ROC-AUC': [roc_auc_score(y_test, y_proba_lr), roc_auc_score(y_test, y_proba_rf)],\n",
    "    'Accuracy': [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_rf)],\n",
    "    'F1 Score': [f1_score(y_test, y_pred_lr), f1_score(y_test, y_pred_rf)],\n",
    "    'Precision': [precision_score(y_test, y_pred_lr), precision_score(y_test, y_pred_rf)],\n",
    "    'Recall': [recall_score(y_test, y_pred_lr), recall_score(y_test, y_pred_rf)]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "ax.plot(fpr_lr, tpr_lr, color='blue', lw=2, \n",
    "        label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_proba_lr):.4f})')\n",
    "\n",
    "# Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "ax.plot(fpr_rf, tpr_rf, color='green', lw=2, \n",
    "        label=f'Random Forest (AUC = {roc_auc_score(y_test, y_proba_rf):.4f})')\n",
    "\n",
    "# Random guess line\n",
    "ax.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guess')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "ax.set_title('ROC Curve Comparison - Baseline Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "We will tune the best performing model (Random Forest) using **RandomizedSearchCV** with a maximum of 3 values per hyperparameter as per project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest (max 3 values per hyperparameter)\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 200, 300],      # Number of trees\n",
    "    'classifier__max_depth': [10, 15, 20],            # Maximum depth of trees\n",
    "    'smote__k_neighbors': [3, 5, 7]                   # SMOTE neighbors\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space:\")\n",
    "for param, values in param_dist.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for tuning\n",
    "pipeline_tune = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Setup cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline_tune,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,              # Number of parameter combinations to try\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"\\nTuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters and score\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest Cross-Validation ROC-AUC: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 parameter combinations\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "cv_results_sorted = cv_results.sort_values('rank_test_score')[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(5)\n",
    "print(\"\\nTop 5 Parameter Combinations:\")\n",
    "print(cv_results_sorted.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION (Tuned Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest ROC-AUC Score: {roc_auc_score(y_test, y_proba_best):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Test F1 Score: {f1_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_pred_best):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_best, display_labels=['No', 'Yes'], \n",
    "                                        cmap='Blues', ax=ax)\n",
    "ax.set_title('Final Model (Tuned Random Forest) - Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final ROC Curve\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_proba_best)\n",
    "ax.plot(fpr_best, tpr_best, color='blue', lw=2, \n",
    "        label=f'Tuned Random Forest (AUC = {roc_auc_score(y_test, y_proba_best):.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guess (AUC = 0.50)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "ax.set_title('ROC Curve - Final Tuned Model', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Model Improvement Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (Baseline)', 'Random Forest (Baseline)', 'Random Forest (Tuned)'],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test, y_proba_lr),\n",
    "        roc_auc_score(y_test, y_proba_rf),\n",
    "        roc_auc_score(y_test, y_proba_best)\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_best)\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        f1_score(y_test, y_pred_lr),\n",
    "        f1_score(y_test, y_pred_rf),\n",
    "        f1_score(y_test, y_pred_best)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Calculate improvement\n",
    "baseline_auc = roc_auc_score(y_test, y_proba_rf)\n",
    "tuned_auc = roc_auc_score(y_test, y_proba_best)\n",
    "improvement = ((tuned_auc - baseline_auc) / baseline_auc) * 100\n",
    "\n",
    "print(f\"\\n✓ Improvement from baseline to tuned: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names after preprocessing\n",
    "feature_names = numerical_cols.copy()\n",
    "\n",
    "# Get categorical feature names from OneHotEncoder\n",
    "ohe = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create DataFrame and sort\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = importance_df.head(15)\n",
    "ax.barh(top_features['Feature'], top_features['Importance'], color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_title('Top 15 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'bank_marketing_model.pkl')\n",
    "print(\"Model saved as 'bank_marketing_model.pkl'\")\n",
    "\n",
    "# Save feature names for Streamlit app\n",
    "joblib.dump({\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'feature_names': feature_names\n",
    "}, 'feature_info.pkl')\n",
    "print(\"Feature info saved as 'feature_info.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Conclusion\n",
    "\n",
    "### Summary\n",
    "- **Problem:** Predicting customer subscription to term deposits in bank marketing campaigns\n",
    "- **Challenge:** Highly imbalanced dataset (~88% No, ~12% Yes)\n",
    "- **Solution:** Used SMOTE to handle class imbalance + Random Forest classifier\n",
    "\n",
    "### Key Results\n",
    "- **Final ROC-AUC Score:** ~0.79 (79% ability to distinguish between subscribers and non-subscribers)\n",
    "- **Key Predictive Features:** \n",
    "  - Economic indicators (euribor3m, nr.employed, emp.var.rate)\n",
    "  - Contact-related features (previous outcome, number of contacts)\n",
    "  - Demographic features (age, job type)\n",
    "\n",
    "### Business Impact\n",
    "- The model can help banks prioritize which customers to contact\n",
    "- Focusing on high-probability customers can improve conversion rates and reduce marketing costs\n",
    "- Economic conditions play a significant role in customer decisions\n",
    "\n",
    "### Limitations & Future Work\n",
    "- Model performance could be improved with additional feature engineering\n",
    "- Consider testing other algorithms (XGBoost, LightGBM)\n",
    "- Regular retraining needed as economic conditions change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
